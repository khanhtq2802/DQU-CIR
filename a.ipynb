{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "import torch, torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[[156, 134, 139,  ..., 161, 170, 170],\n",
      "         [136, 128, 140,  ..., 177, 166, 166],\n",
      "         [129, 131, 137,  ..., 180, 176, 176],\n",
      "         ...,\n",
      "         [195, 200, 208,  ..., 197, 201, 201],\n",
      "         [198, 196, 198,  ..., 200, 192, 192],\n",
      "         [203, 195, 191,  ..., 203, 182, 182]],\n",
      "\n",
      "        [[153, 131, 136,  ..., 150, 160, 160],\n",
      "         [133, 125, 137,  ..., 166, 156, 156],\n",
      "         [126, 128, 134,  ..., 171, 166, 166],\n",
      "         ...,\n",
      "         [195, 200, 208,  ..., 197, 201, 201],\n",
      "         [198, 196, 198,  ..., 199, 191, 191],\n",
      "         [203, 195, 191,  ..., 202, 181, 181]],\n",
      "\n",
      "        [[148, 126, 131,  ..., 146, 158, 158],\n",
      "         [128, 120, 132,  ..., 162, 154, 154],\n",
      "         [121, 123, 129,  ..., 166, 164, 164],\n",
      "         ...,\n",
      "         [195, 200, 208,  ..., 195, 199, 199],\n",
      "         [198, 196, 198,  ..., 197, 189, 189],\n",
      "         [203, 195, 191,  ..., 200, 179, 179]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "image = torchvision.io.read_image('/DQU-CIR/data/CIRR/dev/dev-0-0-img0.png')\n",
    "print(type(image))\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=570x627 at 0x7862F15CECA0>\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "image = Image.open('/DQU-CIR/data/CIRR/dev/dev-0-0-img0.png')\n",
    "print(type(image))\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'shirt'\n",
    "batch_size=16\n",
    "lr=1e-5\n",
    "patience = 5\n",
    "model_name = 'hf-hub:laion/CLIP-ViT-B-16-laion2B-s34B-b88K'\n",
    "pretrained = None\n",
    "hidden_dim=512\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    RandomResizedCrop(size=(224, 224), scale=(0.9, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)\n",
      "    <function _convert_to_rgb at 0x7862f26d19d0>\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      ")\n",
      "Compose(\n",
      "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    <function _convert_to_rgb at 0x7862f26d19d0>\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "clip, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(model_name=model_name,pretrained=pretrained)\n",
    "print(preprocess_train)\n",
    "print(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Lambda()\n",
      "    RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.1, 0.1))\n",
      "    RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)\n",
      "    RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), scale=(0.9, 1.1), shear=[-10.0, 10.0])\n",
      "    RandomApply(\n",
      "    p=0.1\n",
      "    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))\n",
      ")\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "preprocess_train = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0), ratio=(0.75, 1.3333), interpolation=transforms.InterpolationMode.BICUBIC, antialias=True),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                         std=[0.26862954, 0.26130258, 0.27577711])\n",
    "])\n",
    "print(preprocess_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def draw_text(img, point, text):\n",
    "    fontScale = 0.5\n",
    "    color = (0, 255, 0)  # Màu chữ (Xanh dương)\n",
    "    thickness = 2\n",
    "    fontFace = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    # In chữ lên ảnh\n",
    "    cv2.putText(img, str(text), point, fontFace, fontScale, color, thickness, lineType=cv2.LINE_AA)\n",
    "    return img\n",
    "\n",
    "\n",
    "def draw_text_line(img, point, text_line: str):\n",
    "    fontScale = 0.7\n",
    "    # Tách các dòng chữ\n",
    "    lines = text_line.split(\", \")\n",
    "    for i, text in enumerate(lines):\n",
    "        if text:\n",
    "            # Xác định vị trí của từng dòng\n",
    "            draw_point = (point[0], point[1] + int(20 * i * fontScale))\n",
    "            img = draw_text(img, draw_point, text)\n",
    "    return img\n",
    "\n",
    "# Ví dụ sử dụng (màu trắng):\n",
    "img = cv2.imread(\"/DQU-CIR/data/FashionIQ/resized_image/shirt/978980539X.jpg\")  # Thay bằng đường dẫn ảnh của bạn\n",
    "text_to_draw = \"maroon, sports graphic, logo\"\n",
    "point = (50, 50)\n",
    "img = draw_text_line(img, point, text_to_draw)\n",
    "cv2.imwrite(\"/DQU-CIR/B00A0AAD20.jpg\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_train = transforms.Compose([\n",
    "#     transforms.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "#     transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0), ratio=(0.75, 1.3333), interpolation=transforms.InterpolationMode.BICUBIC, antialias=True),\n",
    "#     # transforms.RandomHorizontalFlip(p=0.5),\n",
    "#     # transforms.RandomVerticalFlip(p=0.5),\n",
    "#     # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "#     transforms.RandomRotation(degrees=15),\n",
    "#     transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],std=[0.26862954, 0.26130258, 0.27577711])\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image in list_image:\n",
    "#     old = os.path.join(resized_image_category_path, f\"{image} .jpg\")\n",
    "#     new = os.path.join(resized_image_category_path, f\"{image}.jpg\")\n",
    "#     os.rename(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# for batch in trainloader:\n",
    "#     break\n",
    "# print(batch['candidate_texts'][0])\n",
    "\n",
    "# img = batch['candidate_imgs'][0].permute(1, 2, 0).cpu().numpy()  # Đổi thứ tự kênh từ (C, H, W) -> (H, W, C)\n",
    "# # Hiển thị ảnh\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# plt.imshow(img)\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# img = batch['target_imgs'][0].permute(1, 2, 0).cpu().numpy()  # Đổi thứ tự kênh từ (C, H, W) -> (H, W, C)\n",
    "# # Hiển thị ảnh\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# plt.imshow(img)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# def display_original_image(image_tensor):\n",
    "#     if len(image_tensor.shape) == 3:\n",
    "#         num_channels = image_tensor.shape[0]\n",
    "#         if num_channels == 3:  # Assuming RGB channels\n",
    "#             mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(-1, 1, 1)\n",
    "#             std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(-1, 1, 1)\n",
    "#             denormalized_image = image_tensor.mul(std) + mean\n",
    "#         else:\n",
    "#             raise NotImplementedError(\"Normalization reversal for non-RGB images not implemented yet\")\n",
    "#         denormalized_image = denormalized_image.permute(1, 2, 0).squeeze()  # Rearrange and remove batch dimension\n",
    "#         pil_image = PIL.Image.fromarray(denormalized_image.mul(255).byte().numpy())\n",
    "#         plt.figure(figsize=(6, 6))\n",
    "#         plt.imshow(pil_image)\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         raise ValueError(\"Input image tensor must have 3 dimensions (C, H, W)\")\n",
    "# for batch in trainloader:\n",
    "#     candidate_text = batch['candidate_texts'][0]\n",
    "#     candidate_img = batch['candidate_imgs'][0]\n",
    "#     target_img = batch['target_imgs'][0]\n",
    "#     candidate = batch['candidates'][0]\n",
    "#     target = batch['targets'][0]\n",
    "#     print(f'/DQU-CIR/data/FashionIQ/resized_image/dress/{candidate}.jpg')\n",
    "#     print(f'/DQU-CIR/data/FashionIQ/resized_image/dress/{target}.jpg')\n",
    "#     display_original_image(candidate_img)\n",
    "#     display_original_image(target_img)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([0, 1, 2, 3, 4, 5])\n",
    "mask = torch.tensor([True, False, False, True, False, False])\n",
    "print(x[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[10]])\n",
    "a.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def display_original_image(image_tensor):\n",
    "    if len(image_tensor.shape) == 3:\n",
    "        num_channels = image_tensor.shape[0]\n",
    "        if num_channels == 3:  # Assuming RGB channels\n",
    "            mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(-1, 1, 1)\n",
    "            std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(-1, 1, 1)\n",
    "            denormalized_image = image_tensor.mul(std) + mean\n",
    "        else:\n",
    "            raise NotImplementedError(\"Normalization reversal for non-RGB images not implemented yet\")\n",
    "        denormalized_image = denormalized_image.permute(1, 2, 0).squeeze()  # Rearrange and remove batch dimension\n",
    "        pil_image = Image.fromarray(denormalized_image.mul(255).byte().numpy())\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(pil_image)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        raise ValueError(\"Input image tensor must have 3 dimensions (C, H, W)\")\n",
    "\n",
    "# for batch in devloader:\n",
    "#     image_splits_dataset_path = os.path.join(image_splits_path, f\"split.rc2.dev.json\")\n",
    "#     with open(image_splits_dataset_path) as f:\n",
    "#         image_splits_dataset = json.load(f)\n",
    "#     reference = batch['references'][0]\n",
    "#     target_hard = batch['target_hards'][0]\n",
    "#     reference_path = os.path.join(cirr_path, image_splits_dataset[reference].replace('./', ''))\n",
    "#     target_hard_path = os.path.join(cirr_path, image_splits_dataset[target_hard].replace('./', ''))\n",
    "#     print(reference_path)\n",
    "#     print(target_hard_path)\n",
    "#     print(batch['reference_texts'][0])\n",
    "#     display_original_image(batch['reference_imgs'][0])\n",
    "#     display_original_image(batch['target_hard_imgs'][0])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/DQU-CIR/data/FashionIQ/resized_image/dress/B00EV1B9C2.jpg\n",
      "/DQU-CIR/data/FashionIQ/resized_image/dress/B00BLY2L2E.jpg\n"
     ]
    }
   ],
   "source": [
    "print(\"/DQU-CIR/data/FashionIQ/resized_image/dress/B00EV1B9C2.jpg\") # target\n",
    "print(\"/DQU-CIR/data/FashionIQ/resized_image/dress/B00BLY2L2E.jpg\") #candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/DQU-CIR/data/CIRR/test1/test1-1001-2-img0.png\n"
     ]
    }
   ],
   "source": [
    "print(\"/DQU-CIR/data/CIRR/test1/test1-1001-2-img0.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DQU-CIR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
